import numpy as np

class Neuronio:
    """
    Um neurÃ´nio artificial bÃ¡sico que simula o comportamento de um neurÃ´nio biolÃ³gico.
    Recebe mÃºltiplas entradas, aplica pesos, soma tudo e passa por uma funÃ§Ã£o de ativaÃ§Ã£o.
    """
    
    def __init__(self, n_entradas):
        """
        Inicializa o neurÃ´nio com pesos aleatÃ³rios.
        
        Exemplo: Neuronio(2) cria um neurÃ´nio que aceita 2 entradas
        - self.pesos = [-0.05, 0.12]  (valores aleatÃ³rios pequenos)
        - self.bias = 0.0             (ajuste independente)
        """
        # Pesos aleatÃ³rios pequenos + bias
        self.pesos = np.random.normal(0, 0.1, n_entradas)
        self.bias = 0.0
    
    def sigmoid(self, z):
        """
        FunÃ§Ã£o de ativaÃ§Ã£o sigmoid: converte qualquer nÃºmero real em valor entre 0 e 1.
        
        Exemplos:
        - sigmoid(-5) = 0.007  (quase 0)
        - sigmoid(0)  = 0.5    (meio termo) 
        - sigmoid(5)  = 0.993  (quase 1)
        
        Ã‰ como um "interruptor suave" que decide se o neurÃ´nio "dispara" ou nÃ£o.
        """
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
    
    def forward(self, entradas):
        """
        PropagaÃ§Ã£o para frente: calcula a saÃ­da do neurÃ´nio.
        
        Passos:
        1. Soma ponderada: z = entrada1*peso1 + entrada2*peso2 + bias
        2. AtivaÃ§Ã£o: saÃ­da = sigmoid(z)
        
        Exemplo com entradas=[1, 0], pesos=[0.5, -0.3], bias=0.1:
        - z = 1*0.5 + 0*(-0.3) + 0.1 = 0.6
        - saÃ­da = sigmoid(0.6) = 0.646
        """
        # Converte para array NumPy se necessÃ¡rio
        entradas = np.array(entradas)
        # Soma ponderada + ativaÃ§Ã£o
        z = np.dot(entradas, self.pesos) + self.bias
        return self.sigmoid(z)
    
    def treinar(self, entradas, target, lr=0.5):
        """
        Treina o neurÃ´nio ajustando pesos baseado no erro.
        
        Algoritmo (Gradient Descent):
        1. Faz prediÃ§Ã£o
        2. Calcula erro = target - prediÃ§Ã£o  
        3. Calcula quanto cada peso deve mudar
        4. Atualiza pesos na direÃ§Ã£o que reduz o erro
        
        Exemplo: target=1, prediÃ§Ã£o=0.3
        - erro = 1 - 0.3 = 0.7 (erro positivo = neurÃ´nio deve "disparar mais")
        - aumenta pesos das entradas ativas
        - lr controla velocidade do aprendizado
        """
        # Converte para array NumPy
        entradas = np.array(entradas)
        
        # PrediÃ§Ã£o
        predicao = self.forward(entradas)
        
        # Erro e gradientes
        erro = target - predicao
        derivada = predicao * (1 - predicao)  # derivada sigmoid
        
        # Atualiza pesos
        self.pesos += lr * erro * derivada * entradas
        self.bias += lr * erro * derivada
        
        return erro

# Exemplo: NeurÃ´nio aprendendo funÃ§Ã£o AND
if __name__ == "__main__":
    """
    DemonstraÃ§Ã£o: ensina um neurÃ´nio a simular porta lÃ³gica AND
    
    FunÃ§Ã£o AND:
    - 0 AND 0 = 0 (falso E falso = falso)
    - 0 AND 1 = 0 (falso E verdadeiro = falso)  
    - 1 AND 0 = 0 (verdadeiro E falso = falso)
    - 1 AND 1 = 1 (verdadeiro E verdadeiro = verdadeiro)
    
    O neurÃ´nio vai aprender automaticamente os pesos corretos!
    """
    # Cria neurÃ´nio com 2 entradas
    neuronio = Neuronio(2)
    
    # Dados: funÃ§Ã£o AND
    X = [[0,0], [0,1], [1,0], [1,1]]
    y = [0, 0, 0, 1]
    
    print("ğŸ§  Treinando neurÃ´nio para funÃ§Ã£o AND...")
    
    # Treina 1000 vezes
    for epoca in range(1000):
        for i in range(4):
            neuronio.treinar(X[i], y[i])
    
    # Testa
    print("\nğŸ“Š Resultados:")
    for i in range(4):
        resultado = neuronio.forward(X[i])
        print(f"{X[i][0]} AND {X[i][1]} = {resultado:.3f} ({'âœ…' if (resultado > 0.5) == y[i] else 'âŒ'})")
    
    print(f"\nâš™ï¸ Pesos: {neuronio.pesos}")
    print(f"âš™ï¸ Bias: {neuronio.bias:.3f}")